## 简述

HiveQL是**Hive**的查询语言，它是SQL的一种方言，混合了SQL 92、**MySQL**和**Oracle SQL**语言。

**Hive**提供Shell环境供开发者和**Hive**交互。Hive Shell可运行HiveQL，还可以使用`dfs`命令来访问**Hadoop**的文件系统。



## 数据结构

HiveQL的数据结构有`2`种：

- 基本类型：`BOOLEAN`、`TINYINT`、`SMALLINT`、`INT`、`BIGINT`、`FLOAT`、`DOUBLE`、`DECIMAL`、`STRING`、`VARCHAR`、`CHAR`、`BINARY`、`TIMESTAMP`、`DATE`。
- 复杂类型：`ARRAY`、`MAP`、`STRUCT`、`UNION`。

> `STRING`不需要指定固定长度或最大长度。
>
> `TIMESTAMP`精度为纳秒，未封装时区。
>
> `ARRAY`支持下标，`MAP`支持Key，`STRUCT`支持字段。
>
> 复杂类型允许任意层次的嵌套。



## DDL[[1]](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL)

### 建表

```sql
-- 指定表的类型和表的名称
CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name 
-- 指定字段的名称和类型
[(col_name data_type [COMMENT col_comment], ...)]
-- 对表的说明
[COMMENT table_comment]
-- 指定分区表的字段名称，字段类型
[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
-- 局部排序、分桶
[CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
-- 指定行格式、文件格式
[
[ROW FORMAT row_format] 
[STORED AS file_format]
| STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] 
]
-- 指定数据文件存储在HDFS的什么位置
[LOCATION hdfs_path]
-- 表的属性设置
[TBLPROPERTIES (property_name=property_value, ...)] 
-- 把子查询的结果作为一张新表
[AS select_statement];

-- 从另一个表复制
CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
  LIKE existing_table_or_view_name
  [LOCATION hdfs_path];
```

> **Hive**除了能集成（Integrate）**HDFS**中的数据，还能集成**S3**、**HBase**等系统中的数据。

#### CSV

建立`csv`格式的表：

```sql
-- 方法一
CREATE TABLE my_csv_table (id INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',' 
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE;

-- 方法二
-- 需要先安装OpenCSVSerde
-- 除了可以指定列分隔符、行分隔符，还可以指定引用符、转义符、表头、空值、日期格式等
CREATE TABLE my_csv_table (id INTname STRING
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
  'separatorChar' = ',',
  'quoteChar' = '\"',
  'escapeChar' = '\\'
)
STORED AS TEXTFILE;

-- 想要将csv作为文件扩展名，可以使用LOCATION子句
```

### 外部表和内部表的互转

```sql
-- 外部表转内部表
ALTER TABLE [table_name] SET TBLPROPERTIES('EXTERNAL'='FALSE');
-- 内部表转外部表
ALTER TABLE [table_name] SET TBLPROPERTIES('EXTERNAL'='TRUE');
-- 查看转换是否生效
DESC FORMATTED tmp_conversion;
```

### 删表、清空表

```sql
-- 删除表
DROP TABLE [IF EXISTS] table_name [PURGE];
-- 删除分区
ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...] [IGNORE PROTECTION] [PURGE];
-- 清空表、分区
TRUNCATE [TABLE] table_name [PARTITION partition_spec];
```



## DQL

### 加载数据

```sql
LOAD DATA [LOCAL] INPATH 'file_path' [OVERWRITE] INTO TABLE table_name [PARTITION(partcol=val1,partcol2=val2...)];
```

`[LOCAL]`：是否从本地文件系统寻找文件，默认的是从**HDFS**上寻找文件。

`[OVERWRITE]`：是否覆盖表中已有的数据。

### 排序和聚集

`ORDER BY`用于全局排序，`SORT BY`用于局部排序（每个Reduce产生一个排序文件）。

`DISTRIBUTE BY`用于按指定字段分区：指定字段的具有相同值的记录会被分到同一个Reduce中。

> `DISTRIBUTE BY FLOOR(RAND())`可以合并小文件。

如果`SORT BY`和`DISTRIBUTE BY`的字段相同，可以缩写为`CLUSTER BY`。

> `CLUSTER BY`只能升序，想要倒序只能使用`ORDER BY`。（该说法存疑，未能找到权威出处）

### 关联

`WHERE`可用于对生成的关联表进行过滤，相当于`INNER JOIN`，但**Hive**不会自动将`WHERE`转换为`INNER JOIN`。另外，由于没有`ON`，所以`WHERE`会产生笛卡尔积，使查询不可控，故尽量不要用`WHERE`充当`INNER JOIN`。

`LEFT SEMI JOIN`有以下[特点](https://blog.csdn.net/HappyRocking/article/details/79885071)：

1. <u>右表</u>只能在`ON`设置过滤条件，不能在`WHERE`、`SELECT`中设置。
2. 只在Map阶段传递表的Join Key，因此只返回<u>左表</u>中的字段到`SELECT`。
3. 会对<u>右表</u>进行去重，而`INNER JOIN`等不会对<u>右表</u>去重。换句话说，<u>右表</u>有重复值，`LEFT SEMI JOIN`也只会产生一条记录，而`INNER JOIN`等会产生多条。

正是因为第1、2点，`LEFT SEMI JOIN`可以代替`IN`/`EXISTS`。类似的，`LEFT ANTI JOIN`可以代替`NOT IN`/`NOT EXISTS`。

#### 关联条件

在本文中，关联条件特指`ON`子句中的条件，过滤条件特指`WHERE`子句中的条件。`HAVING`中的条件不在本文的讨论范围内。

关联过程大致为`2`步：

1. **扫描数据**：对左表和右表的数据分别进行扫描，然后装入内存。
2. **拼接记录**：以其中的一张表为基础，遍历该表（暂且称呼该表为主表，另一张表称为副表）。每次遍历时：
   1. 若**副表**中**存在**符合关联条件的记录，则将其悉数返回，然后将**这些**记录逐个与**主表**中的当前记录进行拼接。
   2. 若**副表**中**不存在**符合关联条件的记录，则返回<u>一条全部字段都由`NULL`组成的记录</u>，然后将**这条**记录与**主表**中的当前记录进行拼接。
   3. 若**主表**中的当前记录**不符合**关联条件，则返回<u>一条全部字段都由`NULL`组成的记录</u>，然后将**这条**记录与**主表**中的当前记录进行拼接。
3. **放入结果**：对于`LEFT JION`或`RIGHT JOIN`来说，拼接后的记录会直接放入结果集；对`INNER JOIN`来说，如果记录是由<u>一条全部字段都由`NULL`组成的记录</u>拼接而来，则会丢弃该记录，否则才会放入结果集。

> `FULL JOIN`的关联过程比较特殊，不在此讨论。
>
> 不加`ON`的`INNER JOIN`等同于不加`ON`的`FULL JOIN`。

#### 谓词下推

[谓词下推](https://blog.csdn.net/strongyoung88/article/details/81156271)是指，将部分关联条件和部分过滤条件转移到**扫描数据**阶段来执行，从而缩小了要扫描的范围，进而减少了输入数据的体积和**拼接记录**中的遍历次数，最终减小了内存、带宽、CPU的占用，提高了查询速度。

> 谓词就是关联条件和过滤条件，下推就是转移。

对于不同类型的关联，能下推的谓词不同：

- `INNER JOIN`：所有谓词都能下推。
- `FULL JOIN`：所有谓词都不能下推。
- `LEFT JOIN`：关联条件中关于右表的条件，能下推到右表的**扫描数据**阶段；过滤条件中关于左表的条件，能下推到左表的**扫描数据**阶段。
- `RIGHT JOIN`：关联条件中关于左表的条件，能下推到左表的**扫描数据**阶段；过滤条件中关于右表的条件，能下推到右表的**扫描数据**阶段。

> `LEFT SEMI JOIN`同内关联，`LEFT ANTI JOIN`同左关联。
>
> 无论是关联条件，还是过滤条件，都可以分为`2`种：包含两张表的条件和只包含一张表的条件。

如果谓词中包含不确定函数，则整个关联条件/过滤条件都不会下推。对此，可以使用子查询进行改写。

如果已经使用了子查询，则过滤条件应[尽量](https://blog.csdn.net/yawei_liu1688/article/details/122938424)放在子查询中，而非子查询外，直接手动谓词下推，这样语义更清晰。

### 增强聚合

**Hive**提供了[3种](http://lxw1234.com/archives/2015/04/193.htm)增强聚合（Enhanced Aggregation）功能。

`GROUPING SETS`用于对多个列进行分组，以便在一个查询中按多个维度对数据进行聚合，而不必执行多个单独的聚合查询。

`WITH CUBE`用于根据`GROUP BY`的维度的所有组合进行聚合。

`WITH ROLLUP`是`WITH CUBE`的子集，以最左侧的维度为主，从该维度进行层级聚合。

> `GROUPING SETS`、`WITH CUBE`、`WITH ROLLUP`可通过`GROUPING__ID()`获取当前行所属分组的序号。`GROUPING__ID()`可简写为变量 `GROUPING__ID`。



## 优化

使用`EXPLAIN`可以查看查询执行计划的详情，包括AST、各个**Stage**的信息以及各**Stage**之间的依赖关系。

`EXPLAIN EXTENDED`提供更详细的执行计划。

**Hive**使用基于规则的查询优化器，但也提供基于代价的优化器。

### 数据倾斜⭐

[数据倾斜](https://tech.meituan.com/2016/05/12/spark-tuning-pro.html)是指：Key分布不均匀的表，在Shuffle时会发生大量数据分发到少数几个Reduce中，进而导致这些Reduce的执行时长远超其它Reduce，甚至发生`OutOfMemoryError`。

> 但反过来则不一定成立：部分Reduce运行时长过长、发生`OutOfMemoryError`，不一定是数据倾斜导致的，比如数据量过大直接导致的运行时间长、内存溢出。

Key分布不均匀，可能是<span style=background:#c2e2ff>异常</span>导致的，也可能是数据<span style=background:#c2e2ff>本身</span>的特点。

- 对于前者，可以选择`FILTER()`掉异常数据，也可以选择从根源上避免异常数据。
- 对于后者，只能且只需缓解。
  - 增加Reduce的数量，提高并行度。但效果往往有限。
  - 启用预聚合。
  - 基于随机前缀的两阶段聚合：增加随机前缀进行初步聚合，然后再去掉前缀进行最终聚合。
  - 基于随机前缀和扩容表的`JOIN`：对分布不均的表增加`N`以内的随机前缀，以令其分布均匀，此时分布均匀的表需要配合前者的变化而扩容`N`倍。
  - 将Common Join改写为等价的Map Join。但不是所有`JOIN`都能等价改写。
  - `UNION`：将异常的Key单独处理然后汇总。但不适用于异常Key比较集中的情况。


### 其它技巧

对**MapReduce**、**Tez**的执行参数进行调优。

预聚合。

对文件启用压缩。

分区、分桶。

避免小文件过多。

尽量使用Map Join，而非Common Join。



## 函数

`DESC FUNCTION EXTENDED function_name`：查看函数使用说明。

### 数值计算

[略](https://www.hadoopdoc.com/hive/hive-built-in-function#h2-u6570u503Cu8BA1u7B97)。

### 日期[[2]](https://www.hadoopdoc.com/hive/hive-built-in-function#h2-u65E5u671Fu51FDu6570)

`FROM_UNIXTIME(bigint unixtime[, string format])`：<u>UNIX时间戳</u>（`unixtime`）转为指定格式（Format）的字符串。

`UNIX_TIMESTAMP()`：获取当前<u>UNIX时间戳</u>。

`UNIX_TIMESTAMP(string date)`：将`yyyy-MM-dd HH:mm:ss`格式的字符串（`date`）转为<u>UNIX时间戳</u>

`UNIX_TIMESTAMP(string date, string pattern)`：将指定格式（`pattern`）的字符串（`date`）转换为<u>UNIX时间戳</u>。

 `TO_DATE(string date)`、`YEAR(string date)`、`MONTH(string date)`、`DAY(string date)`、`HOUR(string date)`、`MINUTE(string date)`、`SECOND(string date)`、`WEEKOFYEAR(string date)`：返回字符串（`date`）中的日期、年、月、天、小时、分钟、秒、所属的周。

`DATEDIFF(string enddate, string startdate)`：两个日期字符串相差的天数。

`DATE_ADD(string startdate, int days)`、`date_sub (string startdate, int days)`：日期字符串（`startdate`）增加、减少若干天（`days`）的那天日期。

根据某个日期[求](https://www.jianshu.com/p/a478d94ad3a7)周一至周日的日期（基于取模运算）。

> <u>UNIX时间戳</u>基于`UTC 1970.01.01 00:00:00`到现在的总秒数/毫秒数，与时区无关。

`CURRENT_TIMESTAMP()`：获取当前时间戳，[精确到毫秒](https://blog.csdn.net/lz6363/article/details/90740061)。

`FROM_UTC_TIMESTAMP({any primitive type} timestamp, string timezone)`：将UTC中的时间戳（支持毫秒）转换为给定时区。

`TO_UTC_TIMESTAMP({any primitive type} timestamp, string timezone)`：将给定时区中的时间戳转换为UTC。

> UTC与GMT基本上等同，[相差](https://blog.csdn.net/m0_37773338/article/details/103930045)不超过`0.9秒`。
>
> [世界时区英文缩写](https://blog.csdn.net/unetman/article/details/20910933)。

### 条件判断[[3]](https://www.hadoopdoc.com/hive/hive-built-in-function#h2-u6761u4EF6u51FDu6570)

`IF(boolean testCondition, T valueTrue, T valueFalseOrNull)`：`testCondition`为`TRUE`，则返回`valueTrue`，否则返回`valueFalseOrNull`。

`COALESCE(T v1, T v2, …)`：返回参数中的第一个非空值，若所有值都为`NULL`，那么返回`NULL`。

`NVL(T value, T default_value)`：只有两个参数，功能等同于`COALESCE()`。

`CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END`、`CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END`：可用于行列转置；可嵌套；可用于`SELECT`、`WHERE`、`ON`、`OVER`等子句以及`SUM()`、`MAX()`等聚合函数中。

### 字符串

[略](https://www.hadoopdoc.com/hive/hive-built-in-function#h3-u5B57u7B26u4E32u51FDu6570)。

### 集合[[4]](https://blog.csdn.net/Abysscarry/article/details/81505953)

`COLLECT_LIST(col)`、`COLLECT_SET(col)`：列出该列所有的值，常配合`GROUP BY`使用。只接受基本类型，返回`ARRAY`类型。

`EXPLODE(ARRAY<T> a)`、`EXPLODE(MAP<T key,T value> m)`：将`ARRAY`、`MAP`中的元素输出为单独的行。

> `EXPLODE()`是一种内置的表生成函数（User Defined Table Function，UDTF）。UDTF将一行输入转换为多行输出。UDTF要么与侧视图（[Lateral View](https://cwiki.apache.org/confluence/display/hive/languagemanual+lateralview#LanguageManualLateralView-Example)）一起使用，要么紧跟在`SELECT`后面，而不能与其它字段一起使用。并且，UDTF不可嵌套调用，不可与`GROUP BY`、`SORT BY`、`DISTRIBUTE BY`或`CLUSTER BY`一起使用。除了`EXPLODE()`，内置的UDTF还有`POSEXPLODE()`、`INLINE()`、`TRANSPOSE()`、`STACK()`、`GENERATE()`、`JSON_TUPLE()`等。

### 窗口函数[[5]](https://cwiki.apache.org/confluence/display/hive/languagemanual+windowingandanalytics)[[6]](http://lxw1234.com/archives/tag/hive-window-functions)⭐

#### 语法结构

```sql
WINDOW_FUNCTION(pattern)|ANALYTICS_FUNCTION()|AGGREGATE_FUNCTION(pattern) OVER (
    [PARTITION BY partition_expression, ... ]
    [ORDER BY sort_expression [ASC | DESC], ... ]
    [ROWS|RANGE BETWEEN frame_start AND frame_end]
)
```

若指定了`PARTITION BY`，则窗口大小取决于每个分区，否则，指全部数据。

`ORDER BY`用于排序。

`ROWS|RANGE BETWEEN `用于限定窗口范围，其中`frame_start`、`frame_end`遵循以下格式：

- `n PRECEDING`：往前`n`行。
- `n FOLLOWING`：往后`n`行。
- `CURRENT ROW`：当前行。
- `UNBOUNDED PRECEDING`：窗口最前面的行（起点）。
- `UNBOUNDED FOLLOWING`：窗口最后面的行（终点）。

`ROWS BETWEEN `与`RANGE BETWEEN `类似，前者按照<span style=background:#c2e2ff>行号</span>来确定窗口，后者按照排序列的<span style=background:#c2e2ff>值</span>的范围来确定窗口大小。

- 若指定了`ORDER BY`，但未指定`ROWS|RANGE  BETWEEN `，则窗口范围为从起点到当前行。
- 若`ORDER BY`、`ROWS|RANGE  BETWEEN `均未指定，则窗口大小为起点到终点。

#### 窗口函数

`OVER`前可搭配窗口函数：

- `LAG(value_expression[, offset[, default]])`：获取窗口中当前行之<span style=background:#c2e2ff>后</span>指定偏移量的行的值。
  - `value_expression`：要获取值的列或表达式。
  - `offset`：指定偏移量，若未指定，值为`1`。
  - `default`：当偏移量超出窗口范围时，或者获取的值为`NULL`时，要返回的默认值。
- `LEAD(value_expression[, offset[, default]])`：获取窗口中当前行之<span style=background:#c2e2ff>前</span>指定偏移量的行的值。
  - 参数的作用同`LAG()`。
- `FIRST_VALUE(value_expression[, ignore_nulls])`：获取窗口（截至当前行）中第一行的值。
  - `value_expression`：要获取值的列或表达式。
  - `ignore_nulls`：默认为`FALSE`，不忽略空值。
- `LAST_VALUE(value_expression[, ignore_nulls])`：获取窗口（截至当前行）中最后一行的值。
  - 参数的作用同`FIRST_VALUE()`。


```properties
-- LAG/上/后
-- CURRENT/中/当前行
-- LEAD/下/前
```

窗口函数不支持`ROWS|RANGE BETWEEN `子句。

#### 分析函数

`OVER`前还可搭配分析函数：

- `ROW_NUMBER()`：自增，不重复。
- `RANK()`：自增，值相等时会重复，且产生空位。
- `DENSE_RANK()`：自增，值相等时会重复，不产生空位。
- `NTILE(n)`：将分区中的数据切分为指定数量`n`的桶，并将桶编号附加到每行。如果切分不均匀，则会自动增加第一个分组的大小。
- `CUME_DIST()`：累积分布，Cumulative distribution，<u>小于等于当前值的行数</u>与<u>分区内总行数</u>的比值。
- `PERCENT_RANK()`：<u>前行的`rank值`减一</u>与<u>分区内总行数减一</u>的比值。

分析函数不支持`ROWS|RANGE BETWEEN `子句。

#### 聚合函数

`OVER`前还可搭配`COUNT()`、`SUM()`、`AVG()`、`MIN()`、`MAX()`等聚合函数。

#### 应用

- `MIN(login_time) over (partition by uid) as first_login_time`：某用户的<span style=background:#c2e2ff>首次</span>登录时间。
  - `IF(month = LEFT(first_login_time, 7), 1, 0)`：判断是否为[新用户](https://www.nowcoder.com/practice/1ce93d5cec5c4243930fc5e8efaaca1e)。
  - `SUM(DATEDIFF(active_date, first_login_time) = 1、0)`：计算[新用户次留](https://www.nowcoder.com/practice/1fc0e75f07434ef5ba4f1fb2aa83a450)。
- `MAX(value)、SUM(value) over (order by current_day)`：[截止到当前的](https://www.nowcoder.com/practice/1ce93d5cec5c4243930fc5e8efaaca1e)，单日<span style=background:#c2e2ff>最大</span>值、<span style=background:#c2e2ff>累计</span>值。
- `MAX(value)、SUM(value) over (partition by tag order by start_date rows 6 preceding)`：每类对象[7天内](https://www.nowcoder.com/practice/f90ce4ee521f400db741486209914a11)的<span style=background:#c2e2ff>最大</span>值、<span style=background:#c2e2ff>累计</span>值。
- `DENSE_RANK() over (partition by  uid order by start_date desc) <= 3`：某用户处于激活状态的[近3个月](https://www.nowcoder.com/practice/4a3acb02b34a4ecf9045cefbc05453fa)。
- `PERCENT_RANK() over (order by score) <= 0.5`：[50%的](https://www.nowcoder.com/practice/3e598a2dcd854db8b1a3c48e5904fe1c)。
- `ROW_NUMBER() over (partition by tag order by score desc) <= 3`：每类试卷的[前3名](https://www.nowcoder.com/practice/255aa1863fe14aa88694c09ebbc1dbca)、[第2名](https://www.nowcoder.com/practice/b1e2864271c14b63b0df9fc08b559166)。
- `DATE_SUB(login_date, INTERVAL ROW_NUMBER() over (partition by user_id order by login_date) DAY) as virtual_begin_date`：[连续](https://www.nowcoder.com/practice/e080f8a685bc4af3b47749ca3310f1fd)天数。
  - `ROW_NUMBER() over (partition by uid, virtual_begin_date order by in_date) as group_row_num`：连续签到，[额外奖励](https://www.nowcoder.com/practice/aef5adcef574468c82659e8911bb297f)。
- `SUM(event_type) over (order by end_time, event_type=[1,-1] desc)`：[同时](https://www.nowcoder.com/practice/fe24c93008b84e9592b35faa15755e48)/[同时](https://www.nowcoder.com/practice/f301eccab83c42ab8dab80f28a1eef98)人数。
- `SUM(if_follow=[1,-1]) over (partition by author order by start_time) as follow_count`：每个作者截止到当前时间的累计粉丝数。
  - `FIRST_VALUE(follow_count) over (partition by author, month order by start_time desc) as last_count`：每个作者每个自然月的累计粉丝数。
    - `LAG(last_count, 1, 0) over (partition by author order by month)`：每个作者每个自然月的[涨粉数](https://www.nowcoder.com/practice/d337c95650f640cca29c85201aecff84)（本月的减去上个月的）。
- `LAG(event_time, 1)/LEAD(event_time, 1) over (partition by uid order by event_time) as prevent_time`：某用户某项行为的<span style=background:#c2e2ff>时间间隔</span>。
  - `SUM(TIMESTAMPDIFF(second, prevent_time, event_time) > 1800) over (partition by uid order by event_time) as session_id`：时间间隔超过`30`分钟则为新的<span style=background:#c2e2ff>会话</span>。

