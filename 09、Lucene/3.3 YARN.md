## 简述

**Yarn**（Yet Another Resource Negotiator）将“作业”这一概念换成了Application，以便运行**MapReduce**以外的其它应用，如，一个**Storm**应用。

> DAG，Directed Acyclic Graph，有向无环图。

如[下图](https://blog.csdn.net/suifeng3051/article/details/49486927)所示，**Yarn**拓展了**Hadoop**，使其不仅支持**MapReduce**，还支持**Hive**、**HBase**、**Pig**、**Spark**/**Shark**等应用，即，**YARN**令这些应用互不干扰的运行在同一个**Hadoop**中。

![](../images/9/yarn_function.png)



## 组件

**YARN**主要有以下组件：

<span style=background:#ffb8b8>Resource Manager</span>

- 全局级别的进程。
- **Scheduler**：<span style=background:#ffb8b8>Resource Manager</span>中的一个专门用来进行资源管理的组件，负责分配<span style=background:#f8d2ff>Node Manager</span>上的**Container**。

<span style=background:#f8d2ff>Node Manager</span>

- 节点级别的进程。
- **Container**：对CPU、内存等计算资源的抽象，所有应用都运行在**Container**中。
- **Applicaton Manager**：接收任务，并为应用分配一个**Container**来运行<span style=background:#c9ccff>Application Master</span>，并对其进行监控。
- <span style=background:#f8d2ff>Node Manager</span>会不断向<span style=background:#ffb8b8>Resource Manager</span>发送本地**Container**的使用情况。
- 一个<span style=background:#f8d2ff>Node Manager</span>拥有多个**Container**。

<span style=background:#c9ccff>Application Master</span>

- 应用级别的进程。
- 是对运行在**Yarn**中某个应用的抽象，向<span style=background:#ffb8b8>Resource Manager</span>申请**Containers**，与<span style=background:#f8d2ff>Node Manager</span>交互来执行和监控具体的任务。



## 执行过程

如下图所示：

1. Client向<span style=background:#ffb8b8>Resource Manager</span>提交**Applicaton**。
2. <span style=background:#ffb8b8>Resource Manager</span>寻找一个可运行**Container**的<span style=background:#f8d2ff>Node Manager</span>，并在**Container**中启动一个<span style=background:#c9ccff>Application Master</span>实例。
3. <span style=background:#c9ccff>Application Master</span>启动后，向<span style=background:#ffb8b8>Resource Manager</span>注册，注册后Client就可通过<span style=background:#ffb8b8>Resource Manager</span>获取为其创建的<span style=background:#c9ccff>Application Master</span>的详细信息，然后Client与<span style=background:#c9ccff>Application Master</span>直接交互。
4. <span style=background:#c9ccff>Application Master</span>向<span style=background:#ffb8b8>Resource Manager</span>发送请求**Container**。
5. **Container**被分配完后，<span style=background:#ffb8b8>Resource Manager</span>会请求<span style=background:#f8d2ff>Node Manager</span>启动**Container**。
6. **Container**会把运行过程中进度、状态等信息发送给<span style=background:#c9ccff>Application Master</span>。
7. Client从<span style=background:#c9ccff>Application Master</span>中获取自己提交的应用的进度、运行状态等信息。
8. 当提交的应用执行完成，<span style=background:#c9ccff>Application Master</span>向<span style=background:#ffb8b8>Resource Manager</span>取消注册然后关闭，归还相应的**Container**。

![](../images/9/yarn_node.png)

## 资源隔离

资源隔离有多种实现方式：硬件虚拟化、虚拟机、Cgroups和LinuxContainer等。

**YARN**对内存和CPU采用了[不同的资源隔离方案](https://www.jianshu.com/p/01ff4f60afce)。

### 内存

内存是一种限制性资源，其大小直接决定了应用的死活。

**YARN**提供了2种方案：

1. 基于Cgroups的方案。
   1. Cgroups是Linux内核提供的弹性资源隔离机制，可以<u>严格</u>限制内存使用上限，一旦进程使用资源量超过事先定义的上限值，则可将其杀死。
2. 线程监控方案。
   1. 默认方案。
   2. 在这一方案中，各个Task运行在独立的JVM中以达到资源隔离的目的。由于Task可能会创建子进程，而JVM在创建子进程之后、执行子进程之前会复制一份父进程内存空间，从而导致**Container**的内存使用量翻番，容易导致误判，对此，**YARN**采用了线程监控的方法进行解决：<span style=background:#f8d2ff>Node Manager</span>会启动一个额外的线程来监控**Container**的内存使用量，并结合进程的“年龄”来判断内存使用量是否超标，一旦超标就会将其杀掉。

### CPU

CPU是一种弹性资源，其大小不会影响应用的死活，因此采用了Cgroups。

<span style=background:#f8d2ff>Node Manager</span>默认不开启对CPU的资源隔离。

