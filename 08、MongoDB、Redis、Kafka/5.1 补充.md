
## 目录结构

```java
root
|----commitlog							   # Commit Log
|     |----00000000000000000000			   # 根据Offset命名
|     |----00000000001073741824			   # 每个文件默认大小1GB（1GB = 1073741824B）
|----config								   # 配置文件
|     |----consumerFilter.json
|     |----consumerOffset.json
|     |----delayconsumerOffset.json
|     |----subscriptionGroup.json
|     |----topic.json
|----consumerqueue						   # Consumer Queue
|     |----test-topic
|           |----0						   # Queue ID
|           |    |----00000000000000000000 # 大小约5.72MB，300K条数据
|           |----1
|                |----00000000000012300000
|                |----00000000000045600000
|     |----%RETRY%ConsumerGroupA		   # 消费失败，消息将发往重试队列
|     |----%DLQ%ConsumerGroupA			   # 重试指定次数仍然失败，则发往死信队列（Dead Letter Queue）
|     |----SCHEDULE_TOPIC_XXXX			   # 延迟重试，减轻Broker的负担
|----index								   # 用于索引Message的Key
|     |----202110032340001				   # 以创建时的时间命名，大小固定（420000040Byte），约20M项索引
```

根据Key即可从索引找出指定时间范围内的结果集。

[索引文件结构为](https://blog.csdn.net/quhongwei_zhanqiu/article/details/39153195)：

<table style="font-size: 12px; width:600px">
		<thead>
			<tr>
       <th style="padding: 0 3px; width: 60px;">40 Byte</th>
       <th style="padding: 0 3px; width: 80px;">5M * 4 Byte</th>
       <th style="padding: 0 3px; width: 220px;">20M * 20 Byte</th>
       <th style="padding: 0 3px; width: 240px;">总计420000040 Byte</th>
   </tr>
		</thead>
<tbody>
<tr>
   <td style="padding: 0 3px;">Header</td>
   <td style="padding: 0 3px;">Slot Table</td>
   <td style="padding: 0 3px;">Index Linked List</td>
   <td style="padding: 0 3px;">大小固定</td>
</tr>
</tbody>
</table>


其中“Header”结构为：

<table style="font-size: 12px; width:600px">
		<thead>
			<tr>
  <th style="padding: 0 3px; width: 120px;">8 Byte</th>
  <th style="padding: 0 3px; width: 120px;">8 Byte</th>
  <th style="padding: 0 3px; width: 120px;">8 Byte</th>
  <th style="padding: 0 3px; width: 120px;">8 Byte</th>
  <th style="padding: 0 3px; width: 60px;">4 Byte</th>
  <th style="padding: 0 3px; width: 60px;">4 Byte</th>
</tr>
		</thead>
<tbody>
<tr>
   <td style="padding: 0 3px;">Begin Timestamp</td>
   <td style="padding: 0 3px;">End Timestamp</td>
   <td style="padding: 0 3px;">Begin Physical Offset</td>
   <td style="padding: 0 3px;">End Physical Offset</td>
   <td style="padding: 0 3px;">Hash Slot Count</td>
   <td style="padding: 0 3px;">Index Count</td>
</tr>
</tbody>
</table>


“Slot Table”存储的是它对应的“Index Linked List”的表头，即，采用拉链法解决散列冲突。

“Index Linked List”是由索引项组成的链表，索引项倒序排列，其中的索引项结构为：

<table style="font-size: 12px; width:600px">
		<thead>
			<tr>
  <th style="padding: 0 3px; width: 120px;">4 Byte</th>
  <th style="padding: 0 3px; width: 240px;">8 Byte</th>
  <th style="padding: 0 3px; width: 120px;">4 Byte</th>
  <th style="padding: 0 3px; width: 120px;">4 Byte</th>
</tr>
		</thead>
<tbody>
<tr>
   <td style="padding: 0 3px;">Key Hash</td>
   <td style="padding: 0 3px;">Commit Log Offset</td>
   <td style="padding: 0 3px;">Timestamp</td>
   <td style="padding: 0 3px;">Next Index Offset</td>
</tr>
</tbody>
</table>



## 事务

**RocketMQ**的事务参考了2PC（Prepare、Commit），通过“两条半份”消息来实现的。

![](../images/8/rocketmq_transaction.svg)



## 延迟消息的原理

#### Consumer法

**Producer**将指定的时间段转换成具体的时间戳，然后投递到专用**Topic**。

**Consumer**批量拉取消息后，<span style=background:#c2e2ff>定时</span>轮询本地消息表，然后将到点的**Message**投递到目标主题。

另外：

- 批量拉取是为了让这一段时间内的消息<span style=background:#c2e2ff>有序</span>。
- 不同时间粒度的消息最好分别投放到不同的专用主题，由不同的消费者消费，进而避免消费者本地消息的<span style=background:#c2e2ff>积压</span>。也可用[时间轮](https://cloud.tencent.com/developer/article/1831327)的方式防止积压。（格子、层级）
- **Consumer**需要主动[调用暂停/恢复的API](https://zhuanlan.zhihu.com/p/365802989)，以避免**Broker**认为**Consumer**挂掉。

### Broker法

基于**Consumer**法，**Broker**启动一个专门**Consumer**定时轮询，进行投递。

> **RocketMQ**就是这种。

### Producer法

定时将消息投递到目标主题。

需要本地消息表将未发送的主题<span style=background:#c2e2ff>持久化</span>。



## 与Kafka相比

### 吞吐⭐

**Kafka**高吞吐量、低延迟，号称能处理百万级数据，**RocketMQ**虽然只能处理万级数据，但抗堆积能力强，因为：

1. **Kafka**基于不同的**Partition**，是多文件的、多队列的。

   1. **Topic**越多，**Partition**就越多，文件数就越多，会消耗大量文件句柄，同时会使顺序读写退化为随机读写，造成性能急剧下降。

   2. **Kafka**只能保证一个**Partition**内的顺序消费。

2. **RocketMQ**使用一个**Commit Log**保存消息和多个**Consume Queue**保存索引。

   1. 单一**Commit Log**这一设计，令**RocketMQ**在大量写入时也不会有衰减。
   2. **Consume Queue**虽然有多个，但是体积小，并且使用了**MMap**，即便有大量写入也不会有明显衰退。

> 可采用对消息设立优先级的方式，来保证高优先级的消息不会堆积。

### 功能

**RocketMQ**同样支持异步刷盘、**Replication**、拉取消息、事务，此外：

1. 同步刷盘。

   > 虽然都是基于<span style=background:#c9ccff>Page Cache</span>，[但Kafka没有提供同步刷盘的方法](https://new.qq.com/omn/20201124/20201124A0AGP400.html)，所以有时才需要对<span style=background:#c9ccff>Page Cache</span>调优。

2. 支持推送消息。

3. 支持[定时消息、延时消息](https://github.com/apache/rocketmq/blob/master/docs/cn/features.md#8-定时消息)。

4. 支持全局顺序消费。

5. [提供了RocketMQ-Console来](https://blog.csdn.net/luanlouis/article/details/88078657)监控、管理集群。



## 其它消息队列[[1]](https://www.cnblogs.com/duanxz/p/4610827.html)

### ActiveMQ

**AMQ**支持JMS，对消息幂等支持好。

另一个不同是，**AMQ**使用JDBC进行持久化：

1. 轮询：轮询消息队列表，低效。
2. 锁：降低并发量。
3. 数据增长：数据库难以应对消息、日志这类海量数据。

### RabbitMQ

##### 优势

1. 支持AMQP协议
2. 支持多版本语言客户端
3. 轻量级中间件，易于部署
4. Message会根据路由规则分发到不同的队列中，且路由规则灵活，甚至可以自行实现路由规则，。

##### 不足

1. 对消息堆积的支持并不好。大量消息积压的时候，会导致**RabbitMQ**的性能急剧下降。

2. 性能相较于**RocketMQ**、**Kafka**性能相差了一个数量级。

3. 使用的编程语言Erlang，学习路线陡峭，难以二次扩展。

### Talos

#### 从Kafka说起

小米在2015年[之前使用的是](https://www.infoq.cn/article/p9oj2ns8t1ueacbleltc)**Kafka 0.8**。对小米来说，**Kafka**的存储和计算是<span style=background:#c2e2ff>耦合</span>的，这使得：

- 数据分布不均衡。
- 在扩容或故障转移时需要进行数据迁移。

> <span style=background:#ff8000>注意</span>，扩容后是否需要迁移数据取决于具体场景。

于是小米参考**Kafka**自研了**Talos**。**Talos**将存储<span style=background:#c2e2ff>分离</span>到**HDFS**上，其**Broker**只负责计算，这样：

- 在存储、计算分离的基础上，采用<span style=background:#c2e2ff>一致性散列</span>来进行**Partition**以及<span style=background:#d4fe7f>负载均衡</span>。
- 在扩容或故障转移时，就无需进行数据迁移。

由于采用了存储、计算分离的设计，原本由**Broker**负责的**Replicate**也就变成了由**HDFS**负责，原本的**Primary Partition**选举也变成了同一时刻只能由一个**Broker**进行写入。

> 在默认情况下，**HDFS**的每次写请求都会执行一次`flush()`，而**Talos**采用 将一批写请求合并为一次`flush()`，但响应还是会分成多次进行返回 的方式，来提升吞吐、降低延迟。
>
> **Talos**引入虚拟节点的设计，令<span style=background:#d4fe7f>负载</span>更加<span style=background:#d4fe7f>均衡</span>。

#### 其它特点

**Talos**的**Producer**、**Consumer**可以请求任意的**Broker**：

- 如果所请求的**Partition**由该**Broker**负责，则该**Broker**会直接请求**HDFS**。
- 否则，该**Broker**会将请求重定向到相应的**Broker**上（Client寻址）。

> **Talos**使用**ZooKeeper**来存储部分元数据、控制流信息，如，**Broker**的上下线，Topic DDL相关的操作广播等。

**Talos**采用带**Topic-Partition**偏好的线程池来缓解不同**Topic**之间的影响，采用分离读写线程的方式避免部分竞争。

另外，**Talos**还支持多租户，自带便捷、丰富的指标监控。
