## 简述

HiveQL是**Hive**的查询语言，它是SQL的一种方言，混合了SQL 92、**MySQL**和**Oracle SQL**语言。

**Hive**提供Shell环境供开发者和**Hive**交互。Hive Shell可运行HiveQL，还可以使用`dfs`命令来访问**Hadoop**的文件系统。



## 数据结构

HiveQL的数据结构有`2`种：

- 基本类型：`BOOLEAN`、`TINYINT`、`SMALLINT`、`INT`、`BIGINT`、`FLOAT`、`DOUBLE`、`DECIMAL`、`STRING`、`VARCHAR`、`CHAR`、`BINARY`、`TIMESTAMP`、`DATE`。
- 复杂类型：`ARRAY`、`MAP`、`STRUCT`、`UNION`。

> `STRING`不需要指定固定长度或最大长度。
>
> `TIMESTAMP`精度为纳秒，未封装时区。
>
> `ARRAY`支持下标，`MAP`支持Key，`STRUCT`支持字段。
>
> 复杂类型允许任意层次的嵌套。



## DDL[[1]](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+DDL)

### 建表

```sql
-- 指定表的类型和表的名称
CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name 
-- 指定字段的名称和类型
[(col_name data_type [COMMENT col_comment], ...)]
-- 对表的说明
[COMMENT table_comment]
-- 指定分区表的字段名称，字段类型
[PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
-- 局部排序、分桶
[CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
-- 指定行格式、文件格式
[
[ROW FORMAT row_format] 
[STORED AS file_format]
| STORED BY 'storage.handler.class.name' [WITH SERDEPROPERTIES (...)] 
]
-- 指定数据文件存储在HDFS的什么位置
[LOCATION hdfs_path]
-- 表的属性设置
[TBLPROPERTIES (property_name=property_value, ...)] 
-- 把子查询的结果作为一张新表
[AS select_statement];

-- 从另一个表复制
CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name
  LIKE existing_table_or_view_name
  [LOCATION hdfs_path];
```

#### CSV

建立`csv`格式的表：

```sql
-- 方法一
CREATE TABLE my_csv_table (id INT)
ROW FORMAT DELIMITED 
FIELDS TERMINATED BY ',' 
LINES TERMINATED BY '\n' 
STORED AS TEXTFILE;

-- 方法二
-- 需要先安装OpenCSVSerde
-- 除了可以指定列分隔符、行分隔符，还可以指定引用符、转义符、表头、空值、日期格式等
CREATE TABLE my_csv_table (id INTname STRING
)
ROW FORMAT SERDE 'org.apache.hadoop.hive.serde2.OpenCSVSerde'
WITH SERDEPROPERTIES (
  'separatorChar' = ',',
  'quoteChar' = '\"',
  'escapeChar' = '\\'
)
STORED AS TEXTFILE;

-- 想要将csv作为文件扩展名，可以使用LOCATION子句
```

### 外部表和内部表的互转

```sql
-- 外部表转内部表
ALTER TABLE [table_name] SET TBLPROPERTIES('EXTERNAL'='FALSE');
-- 内部表转外部表
ALTER TABLE [table_name] SET TBLPROPERTIES('EXTERNAL'='TRUE');
-- 查看转换是否生效
DESC FORMATTED tmp_conversion;
```

### 删表、清空表

```sql
-- 删除表
DROP TABLE [IF EXISTS] table_name [PURGE];
-- 删除分区
ALTER TABLE table_name DROP [IF EXISTS] PARTITION partition_spec[, PARTITION partition_spec, ...] [IGNORE PROTECTION] [PURGE];
-- 清空表、分区
TRUNCATE [TABLE] table_name [PARTITION partition_spec];
```



## DQL

### 加载数据

```sql
LOAD DATA [LOCAL] INPATH 'file_path' [OVERWRITE] INTO TABLE table_name [PARTITION(partcol=val1,partcol2=val2...)]
```

`[LOCAL]`：是否从本地文件系统寻找文件，默认的是从**HDFS**上寻找文件。

`[OVERWRITE]`：是否覆盖表中已有的数据。

### 排序和聚集

`ORDER BY`用于全局排序，`SORT BY`用于局部排序（每个Reduce产生一个排序文件）。

`DISTRIBUTE BY`用于按指定字段分区：指定字段的具有相同值的记录会被分到同一个Reduce中。

> `DISTRIBUTE BY floor(rand())`可以合并小文件。

如果`SORT BY`和`DISTRIBUTE BY`的字段相同，可以缩写为`CLUSTER BY`。

> `CLUSTER BY`只能升序，想要倒序只能使用`ORDER BY`。（该说法存疑，未能找到权威出处）

### 连接

`WHERE`可用于对生成的连接表进行过滤，相当于`INNER JOIN`，但**Hive**不会自动将`WHERE`转换为`INNER JOIN`。另外，由于没有`ON`，所以`WHERE`会产生笛卡尔积，使查询不可控，故尽量不要用`WHERE`充当`INNER JOIN`。

### 增强聚合

**Hive**提供了[3种](http://lxw1234.com/archives/2015/04/193.htm)增强聚合（Enhanced Aggregation）功能。

`GROUPING SETS`用于对多个列进行分组，以便在一个查询中按多个维度对数据进行聚合，而不必执行多个单独的聚合查询。

`WITH CUBE`用于根据`GROUP BY`的维度的所有组合进行聚合。

`WITH ROLLUP`是`WITH CUBE`的子集，以最左侧的维度为主，从该维度进行层级聚合。

> `GROUPING SETS`、`WITH CUBE`、`WITH ROLLUP`可通过`grouping__id()`获取当前行所属分组的序号。`grouping__id()`可简写为变量 `GROUPING__ID`。



## 优化

使用`EXPLAIN`可以查看查询执行计划的详情，包括AST、各个**Stage**的信息以及各**Stage**之间的依赖关系。

`EXPLAIN EXTENDED`提供更详细的执行计划。

**Hive**使用基于规则的查询优化器，但也提供基于代价的优化器。

### 数据倾斜

[数据倾斜](https://tech.meituan.com/2016/05/12/spark-tuning-pro.html)是指：Key分布不均匀的表，在Shuffle时会发生大量数据分发到少数几个Reducer中，进而导致这些Reducer的执行时长远超其它Reducer，甚至发生`OutOfMemoryError`。

> 但反过来则不一定成立：部分Reducer运行时长过长、发生`OutOfMemoryError`，不一定是数据倾斜导致的。

Key分布不均匀，可能是异常导致的，也可能是数据本身的特点。

- 对于前者，可以选择`filter()`掉异常数据，也可以选择从根源上避免异常数据。
- 对于后者，只能且只需缓解。
  - 启用预聚合。
  - 基于随机前缀的两阶段聚合：增加随机前缀进行初步聚合，然后再去掉前缀进行最终聚合。适用于异常Key较少的情况。
  - 基于随机前缀和扩容**RDD**的`JOIN`：对分布不均的表增加`N`以内的随机前缀，以令其分布均匀，此时分布均匀的表需要配合前者的变化而扩容`N`倍。适用于异常Key较多的情况。
  - 将Common Join改写为等价的Map Join/Broadcast Join。但不是所有`JOIN`都能等价改写。
  - `UNION`：将异常的Key单独处理然后汇总。对异常Key的处理可采用随机前缀或`JOIN`等价改写。
  - 增加Reducer的数量，提高并行度。但效果往往有限。


### 其它技巧

对**MapReduce**、**Tez**的执行参数进行调优。

预聚合。

对文件启用压缩。

分区、分桶。

避免小文件过多。

尽量使用Map Join，而非Common Join。



## 函数

### 数值计算

[略](https://www.hadoopdoc.com/hive/hive-built-in-function#h2-u6570u503Cu8BA1u7B97)。

### 日期[[2]](https://www.hadoopdoc.com/hive/hive-built-in-function#h2-u65E5u671Fu51FDu6570)

`from_unixtime(bigint unixtime[, string format])`：<u>UNIX时间戳</u>（`unixtime`）转为指定格式（format）的字符串。

`unix_timestamp()`：获取当前时区的<u>UNIX时间戳</u>。

`unix_timestamp(string date)`：将`yyyy-MM-dd HH:mm:ss`格式的字符串（`date`）转为<u>UNIX时间戳</u>

`unix_timestamp(string date, string pattern)`：将指定格式（`pattern`）的字符串（`date`）转换为<u>UNIX时间戳</u>。

 `to_date(string date)`、`year(string date)`、`month(string date)`、`day(string date)`、`hour(string date)`、`minute(string date)`、`second(string date)`、`weekofyear(string date)`：返回字符串（`date`）中的日期、年、月、天、小时、分钟、秒、所属的周。

`datediff(string enddate, string startdate)`：两个日期字符串相差的天数。

`date_add(string startdate, int days)`、`date_sub (string startdate, int days)`：日期字符串（`startdate`）增加、减少若干天（`days`）的那天日期。

根据某个日期[求](https://www.jianshu.com/p/a478d94ad3a7)周一至周日的日期（基于取模运算）。

### 条件判断[[3]](https://www.hadoopdoc.com/hive/hive-built-in-function#h2-u6761u4EF6u51FDu6570)

`if(boolean testCondition, T valueTrue, T valueFalseOrNull)`：`testCondition`为`TRUE`，则返回`valueTrue`，否则返回`valueFalseOrNull`。

`coalesce(T v1, T v2, …)`：返回参数中的第一个非空值，若所有值都为`NULL`，那么返回`NULL`。

`nvl(T value, T default_value)`：只有两个参数，功能等同于`coalesce()`。

`CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END`、`CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END`：可用于行列转置。

### 字符串

[略](https://www.hadoopdoc.com/hive/hive-built-in-function#h3-u5B57u7B26u4E32u51FDu6570)。

### 集合[[4]](https://blog.csdn.net/Abysscarry/article/details/81505953)

`collect_list(col)`、`collect_set(col)`：列出该列所有的值，常配合`GROUP BY`使用。只接受基本类型，返回`ARRAY`类型。

`explode(ARRAY<T> a)`、`explode(MAP<Tkey,Tvalue> m)`：将`ARRAY`、`MAP`中的元素输出为单独的行。

> `explode()`是一种内置的表生成函数（User Defined Table Function，UDTF）。UDTF将一行输入转换为多行输出。UDTF要么与侧视图（[Lateral View](https://cwiki.apache.org/confluence/display/hive/languagemanual+lateralview#LanguageManualLateralView-Example)）一起使用，要么紧跟在`SELECT`后面，而不能与其它字段一起使用。并且，UDTF不可嵌套调用，不可与`GROUP BY`、`SORT BY`、`DISTRIBUTE BY`或`CLUSTER BY`一起使用。除了`explode()`，内置的UDTF还有`posexplode()`、`inline()`、`transpose()`、`stack()`、`generate()`、`json_tuple()`等。

### 窗口函数[[5]](https://cwiki.apache.org/confluence/display/hive/languagemanual+windowingandanalytics)[[6]](http://lxw1234.com/archives/tag/hive-window-functions)

#### 语法结构

```sql
analytics_function|aggregate_function([pattern]) OVER (
    [PARTITION BY partition_expression, ... ]
    [ORDER BY sort_expression [ASC | DESC], ... ]
    [ROWS|RANGE BETWEEN frame_start AND frame_end]
)
```

若指定了`PARTITION BY`，则窗口大小取决于每个分区，否则，指全部数据。

`ROWS|RANGE BETWEEN `用于限定窗口范围，其中`frame_start`、`frame_end`遵循以下格式：

- `n PRECEDING`：往前`n`行。
- `n FOLLOWING`：往后`n`行。
- `CURRENT ROW`：当前行。
- `UNBOUNDED PRECEDING`：窗口最前面的行（起点）。
- `UNBOUNDED FOLLOWING`：窗口最后面的行（终点）。

`ROWS BETWEEN `与`RANGE BETWEEN `类似，前者按照<span style=background:#c2e2ff>行号</span>来确定窗口，后者按照排序列的<span style=background:#c2e2ff>值</span>的范围来确定窗口大小。

- 若指定了`ORDER BY`，但未指定`ROWS|RANGE  BETWEEN `，则窗口范围为从起点到当前行。
- 若`ORDER BY`、`ROWS|RANGE  BETWEEN `均未指定，则窗口大小为起点到终点。

#### 窗口函数

`OVER`前可搭配窗口函数：

- `lead(value_expression[, offset[, default]])`：获取窗口中当前行之<span style=background:#c2e2ff>前</span>指定偏移量的行的值。
  - `value_expression`：要获取值的列或表达式。
  - `offset`：指定偏移量，若未指定，值为`1`。
  - `default`：当偏移量超出窗口范围时，或者获取的值为`NULL`时，要返回的默认值。
- `lag(value_expression[, offset[, default]])`：获取窗口中当前行之<span style=background:#c2e2ff>后</span>指定偏移量的行的值。
  - 参数的作用同`lead()`。
- `first_value(value_expression)`：获取窗口（截至当前行）中第一行的值。
- `last_value(value_expression)`：获取窗口（截至当前行）中最后一行的值。

```properties
-- lag/上/后
-- current/中/当前行
-- lead/下/前
```

窗口函数不支持`ROWS|RANGE BETWEEN `子句。

#### 聚合函数

`OVER`前还可搭配`count()`、`sum()`、`avg()`、`min()`、`max()`等聚合函数。

#### 分析函数

`OVER`前还可搭配分析函数：

- `row_number()`：自增，不重复。
- `rank()`：自增，值相等时会重复，且产生空位。
- `dense_rank()`：自增，值相等时会重复，不产生空位。
- `ntile(n)`：将分区中的数据切分为指定数量`n`的桶，并将桶编号附加到每行。如果切分不均匀，则会自动增加第一个分组的大小。`ntile()`不支持`ROWS|RANGE  BETWEEN `。
- `cume_dist()`：累积分布，Cumulative distribution，<u>小于等于当前值的行数</u>与<u>分区内总行数</u>的比值。
- `percent_rank()`：<u>前行的`rank值`减一</u>与<u>分区内总行数减一</u>的比值。

分析函数不支持`ROWS|RANGE BETWEEN `子句。

#### 应用

`min(login_time) over (partition by uid)`：某用户的<span style=background:#c2e2ff>首次</span>登录时间，进而用于判断是否为<span style=background:#c2e2ff>新</span>用户。

`max(is_null=[0|1]) over (partition by uid)`：某用户是否<span style=background:#c2e2ff>包含</span>`null`记录。

`max(score) over (order by current_day)`：截止到当前的单日<span style=background:#c2e2ff>最高</span>记录。

`sum(size) over (order by current_time)`：截止到当前的<span style=background:#c2e2ff>累计</span>的记录。

`dense_rank() over (partition by  uid order by start_date desc)`：某用户处于<span style=background:#c2e2ff>激活</span>状态的<span style=background:#c2e2ff>近几个</span>月。

`percent_rank() over (order by score) <= 0.4`：`40%`的。

`lead(date, 1) over (partition by uid order by date)`：某用户某项行为的<span style=background:#c2e2ff>时间间隔</span>。

`date_sub(login_date, INTERVAL row_number() over (partition by user_id order by login_date DAY) as virtual_begin_date`：<span style=background:#c2e2ff>连续</span>登录天数。

`sum(event_type) over (order by end_time, event_type=[1,-1] desc)`：<span style=background:#c2e2ff>同时</span>人数。

