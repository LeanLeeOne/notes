## 刷写

1.2.0以前，`flush()`的单位是**Region**，而非单个**MemStore**；1.2.0之后，`flush()`[的粒度细化到](http://hbasefly.com/2017/07/02/hbase-sequenceid/#5.1/12)**MemStore**。

`flush()`时，**Region**会不可用：**Region Server**会对**Region**做Snapshot，对**HLog**进行Checkpoint。

由于采用了刷写的设计，Client查询时会先从**MemStore**中查找，未果后再从**StoreFile**中查找。

[刷写时机](https://cloud.tencent.com/developer/article/1005744)有6种：

1. **MemStore**级别限制：当**Region**中任意一个**MemStore**的大小达到阈值，就会触发`flush()`。
2. **Region**级别限制：当**Region**中所有**MemStore**的大小总和达到了阈值，就会触发`flush()`。
3. **Region Server**级别限制：当一个**Region Server**中所有**MemStore**的大小总和达到了阈值，就会触发`flush()`，将最大的几个**MemStore**刷写，直至所有**MemStore**的大小总和低于另一个阈值。
4. 当**Region Server**中**HLog**数量达到上限时：会选取最早的一个**HLog**对应的一个或多个**Region**进行`flush`。
5. 定期`flush`：默认周期为1小时。为避免所有的**MemStore**同时`flush`，定期`flush`会有20000左右的随机延时。
6. 手动`flush`：用户通过shell命令对表或**Region**进行`flush`。

阈值检查的时机分为3种：

1. 写操作前会检查阈值。
2. 合并、拆分**Region**前会检查阈值。
3. 定时。



## 合并

当满足某些条件时，**Region Server**会将**SotreFile**[合并为更大的](http://hbasefly.com/2016/07/13/hbase-compaction-1/)**StoreFile**：

1. <span style=background:#f8d2ff>Minor Compaction</span>：每次`flush()`前，会检查**StoreFile**的文件数量是否达到阈值，若是，则选取部分相邻的**StoreFile**进行合并。会频繁进行。
2. <span style=background:#ffb8b8>Major Compaction</span>：对整个**Region**中相同**Column Family**的所有**Store**合并成一个，合并时会清理被标记删除的、TTL到期、版本号超过设定版本数的数据，开销大。默认每周自动触发一次；但往往禁用自动触发，选择在空闲时间手动执行。

![](E:\markdown\images\9\hbase-compaction.jpg)

Compaction相关参数可查看[文章](https://blog.csdn.net/shenshouniu/article/details/83902291)。



## 拆分

当满足[拆分策略](https://www.cnblogs.com/duanxz/p/3154487.html)所规定的条件时，**Region Server**就会自动将**Region**拆分为均等的两份。

拆分**Region**前，**Region Server**会先将**Region**下线；拆分完成后，**Region Server**在`hbase:meta`新增**Daughter Region**的信息，即，将**Daughter Region**上线，向**Master**汇报，**Master**会将**Daughter Region**分配到相应的**Region Server**上，实现<span style=background:#d4fe7f>负载均衡</span>。

[拆分过程会在秒级以内，因为](https://www.cnblogs.com/duanxz/p/4516283.html#11/12)只是对**Parent Region**进行了逻辑拆分，没有底层数据的重组，即，**Parent Region**没有被销毁，**Daughter Region**只是简单的对**Parent Region**进行了索引。只有当**Daughter Region**进行<span style=background:#ffb8b8>Major Compaction</span>后，发现不再被**Daughter Region**索引后，才会将其进行清理。

可以看到，拆分会导致**Region**暂时不可用，所以往往会关闭自动拆分，并且在建表时，对**Region**进行<span style=background:#19d02a>预分区</span>。

> 关闭方式为：将`hbase.hregion.max.filesize`设为足够大，并指定拆分策略为**ConstantSizeRegionSplitPolicy**。

### 拆分策略

- ##### ConstantSizeRegionSplitPolicy

  - 0.94 版本之前的默认策略，当一个**Region**的所有**SotreFile**的大小之和达到阈值时就会对其进行拆分。

- ##### IncreasingToUpperBoundRegionSplitPolicy

  - 0.94 版本之后的默认策略，根据`min(r^2 * flushSize, maxFileSize)`确定**Region**的体积阈值。
  - 其中`r`为**Region**的数量。
  - `flushSize`由`hbase.hregion.memstore.flush.size`确定，默认<span style=background:#e6e6e6>134217728</span>（<span style=background:#e6e6e6>128 MB</span>）。
  - `maxFileSize`由`hbase.hregion.max.filesize`确定，默认<span style=background:#e6e6e6>10GB</span>。

- ##### DelimitedKeyPrefixRegionSplitPolicy

  - 以**Rowkey**的前缀为拆分依据，保证相同**RowKey**前缀的数据在同一个**Region**中。
  - 通过<span style=background:#c2e2ff>分隔符</span>来间隔前缀和后缀。

- ##### KeyPrefixRegionSplitPolicy

  - 以**Rowkey**的前缀为拆分依据，保证相同**RowKey**前缀的数据在同一个**Region**中。
  - 通过**Table**的`prefix_split_key_policy.prefix_length`属性，来指定前缀长度。
  - 该策略适合前缀固定的**Rowkey**。
  - 当Table未设置该属性，或该属性不为**Integer**类型时，还是会使用**IncreasingToUpperBoundRegionSplitPolicy**。

> 按前缀拆分的两种策略较少使用。
>
> 技巧：“|”的[ASCII值](https://zh.wikipedia.org/wiki/ASCII#.E5.8F.AF.E6.98.BE.E7.A4.BA.E5.AD.97.E7.AC.A6)为124，“~”的ASCII值为126，大于所有的数字和字母等符号。



## Log Structured Merge Tree

[LSMT](https://cloud.tencent.com/developer/article/1441835)是一种文件组织方式，核心思想是将随机读写改为顺序读写。

**LSMT**以追加的的方式写入数据，包括删除、修改操作，这种方式大大提升了写入速度，但是牺牲了部分读性能。

当然，**LSMT**也对读性能进行了增强，通过划分内存、文件分层（分级）的方式。

> **LSMT**源自Google的BigTable，**HBase**、**Cassandra**均采用了**LSMT**。

### 数据结构

#### MemTable

**MT**使用跳表存储，以保持Key的有序。

> 跳表、B+树、红黑树都是二分查找，但B+树、红黑树的维护成本高，不适合写多读少的场景。

#### Immutable MemTable

当**MT**超过一定大小，将对在内存中冻结，成为不可变的，即**Immutable MemTable**，**IMT**，同时会新建一个**MT**继续进行写入。

#### Sorted String Table

**SST**是**LSMT**的核心数据结构。

**SST**是一个有序、不可变的键值对存储结构，键、值的类型都是任意的字节数组。

**SST**内部由**Block**组成，每块大小通常为64KB，并且在**SST**文件的尾部保存有这些块的索引。

1. 读取**SST**时都是从文件末尾开始读，这样会先读取索引，然后对索引进行二分查找，快速找出数据在磁盘中的偏移量，进而读取数据。
2. 当然内存足够大的话，我们可以通过**MMap**将**SST**映射到内存中，提升查询速度。

把**IMT**给`dump`到磁盘的**SST**层的过程称为<span style=background:#f8d2ff>Minor Compaction</span>。

1. 确切的说是`dump`到L0层，需要注意的是这一层的**SST**是没有进行合并的，所以Key在这一层中会出现重复。

当每层的**SST**超出一定大小或一定数量时，就会进行合并，合并好的**SST**放入下一层，此过程称为<span style=background:#ffb8b8>Major Compaction</span>。

1. 这个阶段会将标记删除、过期、超出版本数量限制的的数据删掉，将多个版本的数据合并。合并策略分为size-tiered和leveled。
2. 而**SST**本身就是有序的，使用归并排序（Merge Sort）能进行高效合并。
3. <span style=background:#ffb8b8>Major Compaction</span>的过程开销巨大，一般会禁用自动（周期性）执行该功能，选择在空闲期间执行。

### 读写过程

**LSMT**的写入过程简单概括成：先写**WAL**，然后写入内存中的**MemTable**，随着写入的进行，**MemTable**会变为**Immutable MemTable**，而后**Immutable MemTable**会`dump`到磁盘中，磁盘中的**SST**会逐渐积累并进行合并。

![](E:\markdown\images\9\log-structured-merg-tree.png)

而**LSMT**的读取过程较为简单，先查内存中的缓存，若未命中则查询磁盘中的**SST**，查询过程中会逐层下沉，直到查询到数据，或全表扫描未果。

即便不出现全局扫描，顺序读要比顺序写略慢（因为写就是写完了，而读要一直查找直到找到符合条件的数据），**LSMT**从4个方面对顺序读进行了优化：

1. 缓存：通过将扫描过的块进行缓存，来提升之后的读速度。

2. 合并：定期合并（瘦身）文件，可以减少扫描的数据量，缩短扫描时间。

3. 压缩：SSTable的压缩不是将整个文件进行压缩，二是根据locality将数据进行分组，按组进行压缩，这样我们在读取数据解压时，只需解压部分，减少开销。

4. Bloom Filter：Bloom Filter可以减少不必要的的扫描。

   > Bloom说Key不存在，那肯定是不存在；Bloom说一个Key存在，那么Key可能存在。

### 写放大

顺序写存在的3个放大问题：

1. 读放大。内存中没有，需要查硬盘。
2. 写放大。写入过程中触发Compact，导致实际写入量放大。
3. 空间放大。数据存在冗余，实际占用空间较大。

